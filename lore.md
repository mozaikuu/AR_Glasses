ğŸ¯ Smart Glasses AI Assistant - Complete Project Walkthrough
ğŸ—ï¸ Project Overview
This is a sophisticated multimodal AI assistant designed specifically for smart glasses, featuring voice-activated interactions, real-time object detection, web search capabilities, and intelligent conversation modes. The system combines cutting-edge AI technologies with an intuitive hands-free interface.
ğŸ›ï¸ Core Architecture
ğŸ“Š High-Level Architecture
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ Streamlit â”‚â”€â”€â”€â”€â”‚ FastAPI â”‚â”€â”€â”€â”€â”‚ Cerebras â”‚â”‚ Frontend â”‚ â”‚ Gateway â”‚ â”‚ LLM API â”‚â”‚ (UI + Wake) â”‚ â”‚ (HTTP Bridge) â”‚ â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â–¼ â–¼ â–¼â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ Wake-Word â”‚ â”‚ MCP Server â”‚ â”‚ Agent Loop â”‚â”‚ Detection â”‚ â”‚ (Tools) â”‚ â”‚ (Reasoning) â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ğŸ”§ Technology Stack
Frontend: Streamlit (Web UI + Wake-Word)
Backend: FastAPI (HTTP Gateway)
AI Model: Cerebras Llama-3.3-70B (Cloud API)
Speech: Google Speech Recognition + OpenAI Whisper (Fallback)
Vision: YOLOv8 (Real-time Object Detection)
Search: DuckDuckGo Search API
Audio: PyAudio + PyGame (Recording & Playback)
Protocol: MCP (Model Context Protocol) for Tool Integration
âš™ï¸ Component Deep Dive

1. ğŸ¤– AI Agent Core (agent/)
   The intelligent brain of the system, implementing two distinct reasoning modes:
   ğŸ“ Agent Loop (agent_loop.py)

# Main reasoning engineasync def agent_loop(client, user_input: str, mode: str, image: str = None): history = [] used_tools = set() for loop_num in range(1, MAX_LOOPS + 1): # Get AI decision (reasoning + tool selection) decision = await decide(user_input, history, used_tools, client, mode, image) if decision["is_satisfied"]: return decision["answer"] # Final answer elif decision["tool"]: # Execute tool and continue reasoning result = await execute_tool(decision["tool"], decision["args"]) history.append(f"Tool result: {result}")

ğŸ¯ Decision Making (api_llm.py)
async def decide(query, history, used_tools, client, mode, image=None): # Build comprehensive prompt with tool information tools_info = "\n".join(f"- {t.name}: {t.description}" for t in client.list_tools()) prompt = f""" You are an intelligent agent running in {mode} mode. Available tools: {tools_info} History: {history} Query: {query} Respond with JSON: {{"reasoning": "...", "tool": "...", "args": {{}}, "is_satisfied": false, "answer": ""}} """ # Get structured response from LLM response = await generate_chat([{"role": "user", "content": prompt}]) return extract_json(response)
ğŸ”€ Operating Modes (modes.py)
Quick Mode: Single-pass responses, fast but limited reasoning
Thinking Mode: Multi-loop reasoning until satisfaction, better for complex tasks 2. ğŸ¤ Speech Processing (tools/speech/)
Multi-layered speech recognition with accuracy fallbacks:
ğŸ¯ Primary: Google Speech Recognition

# Fast, accurate, cloud-basedresult = recognizer.recognize_google(audio_data, language="en-US")

ğŸ”„ Fallback: OpenAI Whisper

# Local, privacy-focused, handles edge casesresult = whisper_model.transcribe(audio_array)["text"]

ğŸ“Š Audio Pipeline
Audio Input â†’ Normalization â†’ Format Conversion â†’ Google API â†’ Success? â†“ No â†“ YesWhisper Fallback â†’ Final Transcription â†’ Result 3. ğŸ‘ï¸ Computer Vision (tools/vision/)
Real-time object detection for smart glasses:
ğŸ¯ YOLO Object Detection (yolo.py)
def infer(): # Load YOLO model model = YOLO('yolo11n.pt') # Capture camera frame cap = cv2.VideoCapture(0) ret, frame = cap.read() # Detect objects results = model(frame) # Extract detections detections = [] for result in results: for box in result.boxes: class_name = model.names[int(box.cls)] confidence = float(box.conf) detections.append(f"{class_name} ({confidence:.1f})") return ", ".join(detections) 4. ğŸ” Web Search (tools/search/)
Intelligent web research capabilities:
ğŸ¦† DuckDuckGo Integration (search_web.py)
def retrieve_web_context(query: str, max_results: int = 5): results = [] with DDGS() as ddgs: for result in ddgs.text(query, max_results=max_results): results.append({ "title": result["title"], "body": result["body"], "url": result["href"] }) return results 5. ğŸš¨ Wake-Word System (tools/wakeword/)
Hands-free activation system:
ğŸ”„ State Machine
class SystemState(Enum): IDLE = "idle" # Listening for wake words ACTIVE = "active" # Processing command PROCESSING = "processing" # Waiting for AI response
ğŸ¤ Continuous Listening
def \_listen_for_wake_words(self): while self.is_running and self.state == SystemState.IDLE: with self.microphone as source: audio = self.recognizer.listen(source, timeout=1, phrase_time_limit=2) # Check for wake words text = self.recognizer.recognize_google(audio).lower() for wake_word in self.wake_words: if wake_word in text and confidence >= self.sensitivity: self.\_play_acknowledgment() # Audio feedback self.\_change_state(SystemState.ACTIVE) self.\_listen_for_command() # Switch to command mode 6. ğŸŒ Server Architecture (server/)
HTTP bridge connecting frontend to AI backend:
ğŸšª FastAPI Gateway (gateway.py)
@app.post("/process")async def process_multimodal(req: MultimodalRequest): # 1. Transcribe audio if provided if req.audio: transcribed_text = transcribe_audio_bytes(audio_bytes, dtype=audio_dtype) # 2. Combine inputs (text + transcribed audio + image) combined_text = f"{req.text} [Voice: {transcribed_text}]" # 3. Call AI with tools result = await generate_chat(messages, max_tokens=512) return {"response": result, "transcription": transcribed_text}
ğŸ”§ MCP Server (server.py)
mcp = FastMCP(name="smart-glasses")@mcp.tool()def VisionDetect() -> str: """Detect objects in camera view using YOLO.""" return infer()@mcp.tool() def search_web(query: str) -> dict: """Perform web search and return results.""" return retrieve_web_context(query) 7. ğŸ¨ User Interface (app.py)
Streamlit-based web interface with real-time features:
ğŸ¯ Wake-Word Integration

# Initialize wake-word systemwakeword_system = create_wakeword_system()# Event-driven UI updateswhile not st.session_state.wakeword_events.empty(): event = st.session_state.wakeword_events.get() if event['type'] == 'command_received': st.session_state.command_text = event['command_text'] process_wakeword_command(event['command_text'])

ğŸ“± UI Components
Wake-Word Status: Real-time system state display
Command Recognition: Shows what was heard
AI Response Display: Formatted output with transcriptions
Manual Controls: Fallback recording options
ğŸ”„ Data Flow Walkthrough
ğŸ¤ Voice Command Flow
User says "Nova, what's the weather?"1. Wake-Word Detection â†“2. Audio Acknowledgment (beep) â†“ 3. Command Listening Mode â†“4. Speech Recognition â†’ "what's the weather" â†“5. UI Update â†’ "ğŸ¤ I heard: 'what's the weather'" â†“6. AI Processing Request â†’ Gateway â†“7. Agent Reasoning â†’ Tool Selection â†“8. Web Search Tool Execution â†“9. Response Formatting â†“10. UI Display â†’ "ğŸ¤– AI Response: [weather info]"
ğŸ“¸ Vision + Text Flow
User: "What animal is this?" + [uploads cat photo]1. Image Processing â†“2. YOLO Object Detection â†’ "cat" â†“3. Agent Reasoning â†’ "User asking about cat" â†“4. Tool Selection â†’ VisionDetect + Web Search â†“5. Combined Results â†“6. Final Answer â†’ "That's a cat! Here's more info..."
ğŸ› ï¸ Development Workflow
ğŸš€ Quick Start Process

# 1. Install dependenciespip install -r requirements.txt# 2. Set API keyexport CEREBRAS_API_KEY="your-key-here"# 3. Start backend server python start_gateway.py# 4. Start frontend (new terminal)streamlit run app.py

ğŸ§ª Testing Infrastructure
Audio Testing: test_microphone.py - Records and transcribes speech
Wake-Word Testing: demo_wakeword.py - Tests voice activation
API Testing: Direct endpoint testing with curl/Postman
Integration Testing: Full end-to-end workflows
ğŸ”§ Configuration Management

# config/settings.py - Centralized configurationAPI_KEY = "csk-..." # Cerebras API keyMODEL_ID = "llama3.3-70b" # AI model selectionAUDIO_SAMPLE_RATE = 44100 # Audio settingsVISION_MODEL_PATH = "src/.../yolo11n.pt" # Model paths

ğŸ“¦ Dependency Management

# requirements.txt - All project dependenciesfastmcp==2.14.1 # MCP protocolfastapi # Web framework streamlit # UI frameworktorch # ML frameworkopenai-whisper # Speech recognitionultralytics # YOLO visionduckduckgo-search # Web searchpyaudio # Audio I/O

ğŸ¯ Key Features & Capabilities
âœ¨ Multimodal Input
Text: Direct typing or voice transcription
Voice: Real-time speech recognition with wake-word activation
Vision: Live camera object detection and image analysis
Combined: Multi-input processing (text + voice + image)
ğŸ§  Intelligent Agent
Context Awareness: Maintains conversation history
Tool Integration: Uses appropriate tools based on query
Reasoning Modes: Quick responses vs. deep analysis
Error Handling: Graceful fallbacks and user feedback
ğŸ”§ Advanced Tools
VisionDetect: Real-time object recognition
search_web: Intelligent web research
Speech Recognition: Multi-engine with accuracy fallbacks
Audio Processing: Professional-grade recording and playback
ğŸ“± User Experience
Hands-Free: Wake-word activation ("Nova", "Hey Nova")
Real-Time Feedback: Live status updates and progress indicators
Professional UI: Clean, intuitive interface
Cross-Platform: Works on desktop and mobile browsers
ğŸ”® Future Enhancements
ğŸš€ Planned Features
GPS Navigation: Real-time location and routing
TTS Output: Voice responses for truly hands-free operation
Multi-Language: Support for additional languages
Offline Mode: Local model fallback when network unavailable
Wearable Integration: Direct smart glasses hardware support
âš¡ Performance Optimizations
Edge Computing: On-device processing for privacy
Model Optimization: Quantized models for faster inference
Caching: Intelligent response caching and context preservation
Streaming: Real-time response streaming
ğŸ‰ Project Impact
This Smart Glasses AI Assistant represents a cutting-edge fusion of:
ğŸ¤– Advanced AI: State-of-the-art language models and computer vision
ğŸ¤ Natural Interaction: Hands-free voice control with wake-word activation
ğŸ”§ Practical Tools: Real-world utility through integrated search and detection
ğŸ“± Modern UX: Intuitive web interface accessible anywhere
The system successfully demonstrates how AI can be seamlessly integrated into daily life through wearable technology, providing intelligent assistance that's both powerful and effortless to use.
Ready for production deployment with professional-grade reliability and user experience! ğŸš€âœ¨
